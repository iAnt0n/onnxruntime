{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNHjAlAzX-O0",
        "outputId": "819dadc4-c685-4be1-b5ad-52985a0acf61"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface torch transformers datasets accelerate optimum\n",
        "!pip install onnxruntime-gpu==1.14.0\n",
        "!pip install --upgrade setuptools pip wheel\n",
        "!pip install nvidia-pyindex\n",
        "\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin \n",
        "\n",
        "!sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub\n",
        "!sudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /\"\n",
        "!sudo apt-get update\n",
        "\n",
        "!sudo apt-get install libcudnn8=8.7.0.84-1+cuda11.8\n",
        "!sudo apt-get install libcudnn8-dev=8.7.0.84-1+cuda11.8\n",
        "\n",
        "!pip install tensorrt==8.5.3.1\n",
        "\n",
        "!export CUDA_PATH=/usr/local/cuda\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.8/lib64:/usr/local/lib/python3.9/dist-packages/tensorrt\n",
        "\n",
        "import tensorrt\n",
        "print(tensorrt.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R002kDpXxHKx",
        "outputId": "db0b7b02-97a7-4655-bc21-e349e65a254c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LgFQEcNGOAE8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "import onnxruntime\n",
        "from optimum.bettertransformer import BetterTransformer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "94k7kIBNjKh7"
      },
      "outputs": [],
      "source": [
        "def setup_pytorch_cpu(model_name):\n",
        "    device = torch.device('cpu')\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    return model, device\n",
        "\n",
        "def setup_pytorch_cuda(model_name):\n",
        "    device = torch.device('cuda')\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    return model, device\n",
        "\n",
        "def setup_pytorch_bettertransformers(model_name):\n",
        "    device = torch.device('cuda')\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    model = BetterTransformer.transform(model)\n",
        "    return model, device\n",
        "\n",
        "def setup_onnx_cpu(model_path):\n",
        "    session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
        "    print(session.get_providers())\n",
        "    return session, torch.device('cpu')\n",
        "\n",
        "def setup_onnx_cuda(model_path):\n",
        "    session = onnxruntime.InferenceSession(model_path, providers=['CUDAExecutionProvider'])\n",
        "    print(session.get_providers())\n",
        "    return session, torch.device('cuda')\n",
        "\n",
        "def setup_onnx_tensorrt(model_path):\n",
        "    session = onnxruntime.InferenceSession(model_path, providers=['TensorrtExecutionProvider'])\n",
        "    print(session.get_providers())\n",
        "    return session, torch.device('cuda')\n",
        "\n",
        "def setup_onnx_openvino(model_path):\n",
        "    !pip install onnxruntime-openvino==1.13.1\n",
        "    session = onnxruntime.InferenceSession(model_path, providers=['OpenVINOExecutionProvider'])\n",
        "    print(session.get_providers())\n",
        "    return session, torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WaKP1Vwqle8R"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from transformers import PreTrainedModel\n",
        "import timeit\n",
        "\n",
        "def measure_inference_time(model, device, input, num_iter):\n",
        "    if isinstance(model, onnxruntime.InferenceSession):\n",
        "        io_binding = None\n",
        "        if device.type == 'cuda':\n",
        "            io_binding = model.io_binding()\n",
        "\n",
        "            datas = []\n",
        "\n",
        "            for k, v in input.items():\n",
        "                datas.append(onnxruntime.OrtValue.ortvalue_from_numpy(v, device.type, 0 if device.index is None else device.index))\n",
        "                io_binding.bind_input(name=k, device_type=datas[-1].device_name(), device_id=(0 if device.index is None else device.index), element_type=v.dtype, shape=datas[-1].shape(), buffer_ptr=datas[-1].data_ptr())\n",
        "\n",
        "            for o in model.get_outputs():\n",
        "                io_binding.bind_output(o.name, device.type)\n",
        "\n",
        "        if io_binding is None:\n",
        "            # warmup\n",
        "            for _ in range(3):\n",
        "                model.run(None, input)\n",
        "            return timeit.timeit(lambda: model.run(None, input), number=num_iter)\n",
        "        else:\n",
        "            # warmup\n",
        "            for _ in range(3):\n",
        "                model.run_with_iobinding(io_binding)\n",
        "            return timeit.timeit(lambda: model.run_with_iobinding(io_binding), number=num_iter)\n",
        "  \n",
        "    elif isinstance(model, PreTrainedModel):\n",
        "        proc_input = {k: torch.from_numpy(v).to(device) for k, v in input.items()}\n",
        "\n",
        "        # warmup\n",
        "        for _ in range(3):\n",
        "            model(**proc_input)\n",
        "\n",
        "        return timeit.timeit(lambda: model(**proc_input), number=num_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FibvmJMb3eh-"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "data = numpy.random.randint(low=0, high=256, size=224 * 224 * 3, dtype=numpy.uint8).reshape(224, 224, 3).astype(numpy.float32)\n",
        "data = {'pixel_values': image_processor.preprocess(data, return_tensors=\"np\")['pixel_values']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5USvl2wbwMIk",
        "outputId": "dfda6a18-f1f5-4f0e-f563-27c9a4e5fd34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "2.9568021059994862\n"
          ]
        }
      ],
      "source": [
        "model, device = setup_pytorch_cpu('google/vit-base-patch16-224')\n",
        "# model, device = setup_pytorch_cuda('google/vit-base-patch16-224')\n",
        "# model, device = setup_pytorch_bettertransformers('google/vit-base-patch16-224')\n",
        "# model, device = setup_onnx_cpu('/content/drive/MyDrive/model.onnx')\n",
        "# model, device = setup_onnx_cuda('/content/drive/MyDrive/model.onnx')\n",
        "# model, device = setup_onnx_cpu('/content/drive/MyDrive/model_opt_no_att.onnx')\n",
        "# model, device = setup_onnx_cpu('/content/drive/MyDrive/model_opt_full.onnx')\n",
        "# model, device = setup_onnx_cuda('/content/drive/MyDrive/model_opt_full.onnx')\n",
        "# model, device = setup_onnx_tensorrt('/content/drive/MyDrive/model.onnx')\n",
        "\n",
        "# runtime restart will be required for openvino because of separate onnxruntime-lib\n",
        "# !pip install onnxruntime-openvino\n",
        "# model, device = setup_onnx_openvino('/content/drive/MyDrive/model.onnx')\n",
        "\n",
        "print(measure_inference_time(model, device, data, 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gHbqCNAgF_J_"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = 32\n",
        "\n",
        "bert_config = BertConfig()\n",
        "input_ids = numpy.random.randint(low=0, high=bert_config.vocab_size - 1, size=(batch_size, sequence_length), dtype=numpy.int64)\n",
        "data = {\"input_ids\": input_ids,\n",
        "          \"attention_mask\": numpy.ones([batch_size, sequence_length], dtype=numpy.int64),\n",
        "          \"token_type_ids\": numpy.zeros([batch_size, sequence_length], dtype=numpy.int64)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HesR7SxH0sK",
        "outputId": "13fcab75-4b48-4813-d8e4-ab539fb1e211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['TensorrtExecutionProvider', 'CPUExecutionProvider']\n",
            "1.1626369360001263\n"
          ]
        }
      ],
      "source": [
        "model, device = setup_pytorch_cpu('bert-base-uncased')\n",
        "# model, device = setup_pytorch_cuda('bert-base-uncased')\n",
        "# model, device = setup_pytorch_bettertransformers('bert-base-uncased')\n",
        "# model, device = setup_onnx_cpu('/content/drive/MyDrive/bert.onnx')\n",
        "# model, device = setup_onnx_cuda('/content/drive/MyDrive/bert.onnx')\n",
        "# model, device = setup_onnx_cpu('/content/drive/MyDrive/bert_opt.onnx')\n",
        "# model, device = setup_onnx_cuda('/content/drive/MyDrive/bert_opt.onnx')\n",
        "# model, device = setup_onnx_tensorrt('/content/drive/MyDrive/bert.onnx')\n",
        "\n",
        "print(measure_inference_time(model, device, data, 1000))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
